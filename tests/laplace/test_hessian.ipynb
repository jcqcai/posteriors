{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101.0000,   5.3003,   3.6100, -15.0475,   5.2662,  18.4212,   2.4490,\n",
      "         -12.4884,   4.4733,  -1.8450,   4.2531],\n",
      "        [  5.3003, 108.0566,  -5.4149,  -5.1864,  -5.6259,  -2.9519,  -1.0276,\n",
      "           2.4567,  22.9299,  -5.7365,   5.1043],\n",
      "        [  3.6100,  -5.4149, 117.0197,  -7.5019,  35.3815,  12.1161,   4.0535,\n",
      "           4.4423,   6.7399,  -4.9319, -19.8263],\n",
      "        [-15.0475,  -5.1864,  -7.5019,  70.4873,  -0.6540,  -2.1473,  -3.5546,\n",
      "         -24.9956,  -8.7571,  -2.9979,  -0.4580],\n",
      "        [  5.2662,  -5.6259,  35.3815,  -0.6540,  97.8591,   3.1908,   4.8563,\n",
      "           3.0504,  -7.1151,  -3.2673, -19.2185],\n",
      "        [ 18.4212,  -2.9519,  12.1161,  -2.1473,   3.1908,  84.9926,  18.7322,\n",
      "           0.2138,   5.1761,   7.1025,  -6.9943],\n",
      "        [  2.4490,  -1.0276,   4.0535,  -3.5546,   4.8563,  18.7322,  87.4687,\n",
      "           4.5106,   4.3176,  14.7430,  11.8299],\n",
      "        [-12.4884,   2.4567,   4.4423, -24.9956,   3.0504,   0.2138,   4.5106,\n",
      "         104.6368,   6.0046,   1.2427,   7.9311],\n",
      "        [  4.4733,  22.9299,   6.7399,  -8.7571,  -7.1151,   5.1761,   4.3176,\n",
      "           6.0046, 107.0322,  -4.9620,  10.7736],\n",
      "        [ -1.8450,  -5.7365,  -4.9319,  -2.9979,  -3.2673,   7.1025,  14.7430,\n",
      "           1.2427,  -4.9620,  83.8916,  -0.3128],\n",
      "        [  4.2531,   5.1043, -19.8263,  -0.4580, -19.2185,  -6.9943,  11.8299,\n",
      "           7.9311,  10.7736,  -0.3128, 120.2813]])\n",
      "tensor([[100.9999,   5.3003,   3.6100, -15.0475,   5.2662,  18.4213,   2.4490,\n",
      "         -12.4884,   4.4733,  -1.8450,   4.2531],\n",
      "        [  5.3003, 108.0566,  -5.4149,  -5.1864,  -5.6259,  -2.9519,  -1.0276,\n",
      "           2.4567,  22.9299,  -5.7365,   5.1043],\n",
      "        [  3.6100,  -5.4149, 117.0197,  -7.5019,  35.3815,  12.1161,   4.0535,\n",
      "           4.4423,   6.7399,  -4.9319, -19.8263],\n",
      "        [-15.0475,  -5.1864,  -7.5019,  70.4873,  -0.6540,  -2.1473,  -3.5546,\n",
      "         -24.9956,  -8.7571,  -2.9979,  -0.4580],\n",
      "        [  5.2662,  -5.6259,  35.3815,  -0.6540,  97.8591,   3.1908,   4.8563,\n",
      "           3.0504,  -7.1151,  -3.2673, -19.2185],\n",
      "        [ 18.4213,  -2.9519,  12.1161,  -2.1473,   3.1908,  84.9926,  18.7322,\n",
      "           0.2138,   5.1761,   7.1025,  -6.9943],\n",
      "        [  2.4490,  -1.0276,   4.0535,  -3.5546,   4.8563,  18.7322,  87.4687,\n",
      "           4.5106,   4.3176,  14.7430,  11.8299],\n",
      "        [-12.4884,   2.4567,   4.4423, -24.9956,   3.0504,   0.2138,   4.5106,\n",
      "         104.6368,   6.0046,   1.2427,   7.9311],\n",
      "        [  4.4733,  22.9299,   6.7399,  -8.7571,  -7.1151,   5.1761,   4.3176,\n",
      "           6.0046, 107.0321,  -4.9620,  10.7736],\n",
      "        [ -1.8450,  -5.7365,  -4.9319,  -2.9979,  -3.2673,   7.1025,  14.7430,\n",
      "           1.2427,  -4.9620,  83.8916,  -0.3128],\n",
      "        [  4.2531,   5.1043, -19.8263,  -0.4580, -19.2185,  -6.9943,  11.8299,\n",
      "           7.9311,  10.7736,  -0.3128, 120.2813]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/debiansigma/repos/posteriors/tests/laplace\")\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.func import functional_call, hessian\n",
    "from optree import tree_map\n",
    "from optree.integration.torch import tree_ravel\n",
    "\n",
    "from posteriors import tree_size, diag_normal_log_prob\n",
    "from posteriors.laplace import dense_hessian\n",
    "\n",
    "from scenarios import TestModel\n",
    "\n",
    "\n",
    "def normal_log_likelihood(y, y_pred):\n",
    "    return (\n",
    "        Normal(y_pred, 1, validate_args=False).log_prob(y).sum(dim=-1)\n",
    "    )  # validate args introduces control flows not yet supported in torch.func.vmap\n",
    "\n",
    "\n",
    "def log_posterior_n(params, batch, model, n_data):\n",
    "    y_pred = functional_call(model, params, batch[0])\n",
    "    return diag_normal_log_prob(params, mean=0.0, sd_diag=1.0) + normal_log_likelihood(\n",
    "        batch[1], y_pred\n",
    "    ) * n_data, torch.tensor([])\n",
    "\n",
    "\n",
    "def test_dense_fisher_vmap():\n",
    "    torch.manual_seed(42)\n",
    "    model = TestModel()\n",
    "\n",
    "    xs = torch.randn(100, 10)\n",
    "    ys = model(xs)\n",
    "\n",
    "    batch_size=2\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        TensorDataset(xs, ys),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    def log_posterior(p, b):\n",
    "        return log_posterior_n(p, b, model, len(xs))[0].mean(), torch.tensor([])\n",
    "\n",
    "    params = dict(model.named_parameters())\n",
    "\n",
    "    # Test inplace = False\n",
    "    transform = dense_hessian.build(log_posterior)\n",
    "    laplace_state = transform.init(params)\n",
    "    laplace_state_prec_init = laplace_state.prec\n",
    "    for batch in dataloader:\n",
    "        laplace_state = transform.update(laplace_state, batch, rescale=batch_size/xs.size()[0], inplace=False)\n",
    "\n",
    "    flat_params, params_unravel = tree_ravel(params)\n",
    "    \n",
    "    num_params = tree_size(params)\n",
    "    expected = torch.zeros((num_params, num_params))\n",
    "    for x, y in zip(xs, ys):\n",
    "        with torch.no_grad():\n",
    "            neg_log_p = lambda p: - log_posterior(params_unravel(p), (x,y))[0]\n",
    "            hess = hessian(neg_log_p)(flat_params)\n",
    "        expected += hess / xs.size()[0]\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state.prec, atol=1e-5)\n",
    "    assert not torch.allclose(laplace_state.prec, laplace_state_prec_init)\n",
    "    print(expected)\n",
    "    print(laplace_state.prec)\n",
    "\n",
    "    # Also check full batch\n",
    "    laplace_state_fb = transform.init(params)\n",
    "    laplace_state_fb = transform.update(laplace_state_fb, (xs, ys))\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state_fb.prec, atol=1e-5)\n",
    "\n",
    "    # Test inplace = True\n",
    "    transform = dense_hessian.build(log_posterior)\n",
    "    laplace_state = transform.init(params)\n",
    "    laplace_state_prec_diag_init = laplace_state.prec\n",
    "    for batch in dataloader:\n",
    "        laplace_state = transform.update(laplace_state, batch, rescale=batch_size/xs.size()[0], inplace=True)\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state.prec, atol=1e-5)\n",
    "    assert torch.allclose(laplace_state.prec, laplace_state_prec_diag_init, atol=1e-5)\n",
    "\n",
    "    # Test sampling\n",
    "    num_samples = 10000\n",
    "    laplace_state.prec.data += 0.1 * torch.eye(\n",
    "        num_params\n",
    "    )  # regularize to ensure PSD and reduce variance\n",
    "\n",
    "    mean_copy = tree_map(lambda x: x.clone(), laplace_state.params)\n",
    "    sd_flat = torch.diag(torch.linalg.inv(laplace_state.prec)).sqrt()\n",
    "\n",
    "    samples = dense_hessian.sample(laplace_state, (num_samples,))\n",
    "\n",
    "    samples_mean = tree_map(lambda x: x.mean(dim=0), samples)\n",
    "    samples_sd = tree_map(lambda x: x.std(dim=0), samples)\n",
    "    samples_sd_flat = tree_ravel(samples_sd)[0]\n",
    "\n",
    "    for key in samples_mean:\n",
    "        assert samples[key].shape[0] == num_samples\n",
    "        assert samples[key].shape[1:] == samples_mean[key].shape\n",
    "        assert torch.allclose(samples_mean[key], laplace_state.params[key], atol=1e-1)\n",
    "        assert torch.allclose(mean_copy[key], laplace_state.params[key])\n",
    "\n",
    "    assert torch.allclose(sd_flat, samples_sd_flat, atol=1e-1)\n",
    "\n",
    "\n",
    "test_dense_fisher_vmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eee\n",
      "\n",
      "tensor([[ 7.5550,  6.6453,  7.2144, -2.0363,  7.9845, -1.9044,  1.7539, -4.2317,\n",
      "          5.1046,  7.6623, -6.3767],\n",
      "        [ 6.6453,  5.8452,  6.3457, -1.7911,  7.0231, -1.6751,  1.5428, -3.7222,\n",
      "          4.4900,  6.7397, -5.6089],\n",
      "        [ 7.2144,  6.3457,  6.8891, -1.9445,  7.6245, -1.8186,  1.6749, -4.0409,\n",
      "          4.8745,  7.3168, -6.0892],\n",
      "        [-2.0363, -1.7911, -1.9445,  0.5488, -2.1520,  0.5133, -0.4727,  1.1406,\n",
      "         -1.3758, -2.0652,  1.7187],\n",
      "        [ 7.9845,  7.0231,  7.6245, -2.1520,  8.4384, -2.0127,  1.8536, -4.4723,\n",
      "          5.3948,  8.0979, -6.7392],\n",
      "        [-1.9044, -1.6751, -1.8186,  0.5133, -2.0127,  0.4801, -0.4421,  1.0667,\n",
      "         -1.2868, -1.9315,  1.6074],\n",
      "        [ 1.7539,  1.5428,  1.6749, -0.4727,  1.8536, -0.4421,  0.4072, -0.9824,\n",
      "          1.1851,  1.7788, -1.4804],\n",
      "        [-4.2317, -3.7222, -4.0409,  1.1406, -4.4723,  1.0667, -0.9824,  2.3703,\n",
      "         -2.8592, -4.2918,  3.5717],\n",
      "        [ 5.1046,  4.4900,  4.8745, -1.3758,  5.3948, -1.2868,  1.1851, -2.8592,\n",
      "          3.4490,  5.1771, -4.3085],\n",
      "        [ 7.6623,  6.7397,  7.3168, -2.0652,  8.0979, -1.9315,  1.7788, -4.2918,\n",
      "          5.1771,  7.7711, -6.4672],\n",
      "        [-6.3767, -5.6089, -6.0892,  1.7187, -6.7392,  1.6074, -1.4804,  3.5717,\n",
      "         -4.3085, -6.4672,  5.3821]])\n",
      "ppp\n",
      "\n",
      "tensor([[ 7.5550,  6.6453,  7.2144, -2.0363,  7.9845, -1.9044,  1.7539, -4.2317,\n",
      "          5.1046,  7.6623, -6.3767],\n",
      "        [ 6.6453,  5.8452,  6.3457, -1.7911,  7.0231, -1.6751,  1.5428, -3.7222,\n",
      "          4.4900,  6.7397, -5.6089],\n",
      "        [ 7.2144,  6.3457,  6.8891, -1.9445,  7.6245, -1.8186,  1.6749, -4.0409,\n",
      "          4.8745,  7.3168, -6.0892],\n",
      "        [-2.0363, -1.7911, -1.9445,  0.5488, -2.1520,  0.5133, -0.4727,  1.1406,\n",
      "         -1.3758, -2.0652,  1.7187],\n",
      "        [ 7.9845,  7.0231,  7.6245, -2.1520,  8.4384, -2.0127,  1.8536, -4.4723,\n",
      "          5.3948,  8.0979, -6.7392],\n",
      "        [-1.9044, -1.6751, -1.8186,  0.5133, -2.0127,  0.4801, -0.4421,  1.0667,\n",
      "         -1.2868, -1.9315,  1.6074],\n",
      "        [ 1.7539,  1.5428,  1.6749, -0.4727,  1.8536, -0.4421,  0.4072, -0.9824,\n",
      "          1.1851,  1.7788, -1.4804],\n",
      "        [-4.2317, -3.7222, -4.0409,  1.1406, -4.4723,  1.0667, -0.9824,  2.3703,\n",
      "         -2.8592, -4.2918,  3.5717],\n",
      "        [ 5.1046,  4.4900,  4.8745, -1.3758,  5.3948, -1.2868,  1.1851, -2.8592,\n",
      "          3.4490,  5.1771, -4.3085],\n",
      "        [ 7.6623,  6.7397,  7.3168, -2.0652,  8.0979, -1.9315,  1.7788, -4.2918,\n",
      "          5.1771,  7.7711, -6.4672],\n",
      "        [-6.3767, -5.6089, -6.0892,  1.7187, -6.7392,  1.6074, -1.4804,  3.5717,\n",
      "         -4.3085, -6.4672,  5.3821]])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.func import functional_call\n",
    "from optree import tree_map\n",
    "from optree.integration.torch import tree_ravel\n",
    "\n",
    "from posteriors import tree_size, empirical_fisher, diag_normal_log_prob\n",
    "from posteriors.laplace import dense_fisher\n",
    "\n",
    "from scenarios import TestModel\n",
    "\n",
    "\n",
    "def normal_log_likelihood(y, y_pred):\n",
    "    return (\n",
    "        Normal(y_pred, 1, validate_args=False).log_prob(y).sum(dim=-1)\n",
    "    )  # validate args introduces control flows not yet supported in torch.func.vmap\n",
    "\n",
    "\n",
    "def log_posterior_n(params, batch, model, n_data):\n",
    "    y_pred = functional_call(model, params, batch[0])\n",
    "    return diag_normal_log_prob(params, mean=0.0, sd_diag=1.0) + normal_log_likelihood(\n",
    "        batch[1], y_pred\n",
    "    ) * n_data, torch.tensor([])\n",
    "\n",
    "\n",
    "def test_dense_fisher_vmap():\n",
    "    torch.manual_seed(42)\n",
    "    model = TestModel()\n",
    "\n",
    "    xs = torch.randn(100, 10)\n",
    "    ys = model(xs)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        TensorDataset(xs, ys),\n",
    "        batch_size=2,\n",
    "    )\n",
    "\n",
    "    def log_posterior(p, b):\n",
    "        return log_posterior_n(p, b, model, len(xs))[0].mean(), torch.tensor([])\n",
    "\n",
    "    log_posterior_per_sample = torch.vmap(log_posterior, in_dims=(None, 0))\n",
    "\n",
    "    params = dict(model.named_parameters())\n",
    "\n",
    "    # Test inplace = False\n",
    "    transform = dense_fisher.build(log_posterior)\n",
    "    laplace_state = transform.init(params)\n",
    "    laplace_state_prec_init = laplace_state.prec\n",
    "    for batch in dataloader:\n",
    "        laplace_state = transform.update(laplace_state, batch, inplace=False)\n",
    "\n",
    "    num_params = tree_size(params)\n",
    "    expected = torch.zeros((num_params, num_params))\n",
    "    for x, y in zip(xs, ys):\n",
    "        x = x.unsqueeze(0)\n",
    "        y = y.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            fisher = empirical_fisher(\n",
    "                lambda p: log_posterior_per_sample(p, (x, y)),\n",
    "                has_aux=True,\n",
    "                normalize=False,\n",
    "            )(params)[0]\n",
    "\n",
    "        expected += fisher\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state.prec, atol=1e-5)\n",
    "    assert not torch.allclose(laplace_state.prec, laplace_state_prec_init)\n",
    "\n",
    "    print(\"eee\\n\")\n",
    "    print(expected)\n",
    "    print(\"ppp\\n\")\n",
    "    print(laplace_state.prec)\n",
    "\n",
    "    # Also check full batch\n",
    "    laplace_state_fb = transform.init(params)\n",
    "    laplace_state_fb = transform.update(laplace_state_fb, (xs, ys))\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state_fb.prec, atol=1e-5)\n",
    "\n",
    "    #  Test per_sample\n",
    "    log_posterior_per_sample = partial(log_posterior_n, model=model, n_data=len(xs))\n",
    "    transform_ps = dense_fisher.build(log_posterior_per_sample, per_sample=True)\n",
    "    laplace_state_ps = transform_ps.init(params)\n",
    "    for batch in dataloader:\n",
    "        laplace_state_ps = transform_ps.update(\n",
    "            laplace_state_ps,\n",
    "            batch,\n",
    "        )\n",
    "\n",
    "    assert torch.allclose(laplace_state_ps.prec, laplace_state_fb.prec, atol=1e-5)\n",
    "\n",
    "    # Test inplace = True\n",
    "    transform = dense_fisher.build(log_posterior)\n",
    "    laplace_state = transform.init(params)\n",
    "    laplace_state_prec_diag_init = laplace_state.prec\n",
    "    for batch in dataloader:\n",
    "        laplace_state = transform.update(laplace_state, batch, inplace=True)\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state.prec, atol=1e-5)\n",
    "    assert torch.allclose(laplace_state.prec, laplace_state_prec_diag_init, atol=1e-5)\n",
    "\n",
    "    # Test sampling\n",
    "    num_samples = 10000\n",
    "    laplace_state.prec.data += 0.1 * torch.eye(\n",
    "        num_params\n",
    "    )  # regularize to ensure PSD and reduce variance\n",
    "\n",
    "    mean_copy = tree_map(lambda x: x.clone(), laplace_state.params)\n",
    "    sd_flat = torch.diag(torch.linalg.inv(laplace_state.prec)).sqrt()\n",
    "\n",
    "    samples = dense_fisher.sample(laplace_state, (num_samples,))\n",
    "\n",
    "    samples_mean = tree_map(lambda x: x.mean(dim=0), samples)\n",
    "    samples_sd = tree_map(lambda x: x.std(dim=0), samples)\n",
    "    samples_sd_flat = tree_ravel(samples_sd)[0]\n",
    "\n",
    "    for key in samples_mean:\n",
    "        assert samples[key].shape[0] == num_samples\n",
    "        assert samples[key].shape[1:] == samples_mean[key].shape\n",
    "        assert torch.allclose(samples_mean[key], laplace_state.params[key], atol=1e-1)\n",
    "        assert torch.allclose(mean_copy[key], laplace_state.params[key])\n",
    "\n",
    "    assert torch.allclose(sd_flat, samples_sd_flat, atol=1e-1)\n",
    "test_dense_fisher_vmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eee\n",
      "\n",
      "tensor([[100.0000,   5.3003,   3.6100, -15.0475,   5.2662,  18.4212,   2.4490,\n",
      "         -12.4884,   4.4733,  -1.8450,   4.2531],\n",
      "        [  5.3003, 107.0566,  -5.4149,  -5.1864,  -5.6259,  -2.9519,  -1.0276,\n",
      "           2.4567,  22.9299,  -5.7365,   5.1043],\n",
      "        [  3.6100,  -5.4149, 116.0197,  -7.5019,  35.3815,  12.1161,   4.0535,\n",
      "           4.4423,   6.7399,  -4.9319, -19.8263],\n",
      "        [-15.0475,  -5.1864,  -7.5019,  69.4874,  -0.6540,  -2.1473,  -3.5546,\n",
      "         -24.9956,  -8.7571,  -2.9979,  -0.4580],\n",
      "        [  5.2662,  -5.6259,  35.3815,  -0.6540,  96.8592,   3.1908,   4.8563,\n",
      "           3.0504,  -7.1151,  -3.2673, -19.2185],\n",
      "        [ 18.4212,  -2.9519,  12.1161,  -2.1473,   3.1908,  83.9926,  18.7322,\n",
      "           0.2138,   5.1761,   7.1025,  -6.9943],\n",
      "        [  2.4490,  -1.0276,   4.0535,  -3.5546,   4.8563,  18.7322,  86.4687,\n",
      "           4.5106,   4.3176,  14.7430,  11.8299],\n",
      "        [-12.4884,   2.4567,   4.4423, -24.9956,   3.0504,   0.2138,   4.5106,\n",
      "         103.6367,   6.0046,   1.2427,   7.9311],\n",
      "        [  4.4733,  22.9299,   6.7399,  -8.7571,  -7.1151,   5.1761,   4.3176,\n",
      "           6.0046, 106.0322,  -4.9620,  10.7736],\n",
      "        [ -1.8450,  -5.7365,  -4.9319,  -2.9979,  -3.2673,   7.1025,  14.7430,\n",
      "           1.2427,  -4.9620,  82.8916,  -0.3128],\n",
      "        [  4.2531,   5.1043, -19.8263,  -0.4580, -19.2185,  -6.9943,  11.8299,\n",
      "           7.9311,  10.7736,  -0.3128, 119.2813]])\n",
      "ppp\n",
      "\n",
      "tensor([[100.0000,   5.3003,   3.6100, -15.0475,   5.2662,  18.4212,   2.4490,\n",
      "         -12.4884,   4.4733,  -1.8450,   4.2531],\n",
      "        [  5.3003, 107.0566,  -5.4149,  -5.1864,  -5.6259,  -2.9519,  -1.0276,\n",
      "           2.4567,  22.9299,  -5.7365,   5.1043],\n",
      "        [  3.6100,  -5.4149, 116.0197,  -7.5019,  35.3815,  12.1161,   4.0535,\n",
      "           4.4423,   6.7399,  -4.9319, -19.8263],\n",
      "        [-15.0475,  -5.1864,  -7.5019,  69.4874,  -0.6540,  -2.1473,  -3.5546,\n",
      "         -24.9956,  -8.7571,  -2.9979,  -0.4580],\n",
      "        [  5.2662,  -5.6259,  35.3815,  -0.6540,  96.8591,   3.1908,   4.8563,\n",
      "           3.0504,  -7.1151,  -3.2673, -19.2185],\n",
      "        [ 18.4212,  -2.9519,  12.1161,  -2.1473,   3.1908,  83.9926,  18.7322,\n",
      "           0.2138,   5.1761,   7.1025,  -6.9943],\n",
      "        [  2.4490,  -1.0276,   4.0535,  -3.5546,   4.8563,  18.7322,  86.4687,\n",
      "           4.5106,   4.3176,  14.7430,  11.8299],\n",
      "        [-12.4884,   2.4567,   4.4423, -24.9956,   3.0504,   0.2138,   4.5106,\n",
      "         103.6368,   6.0046,   1.2427,   7.9311],\n",
      "        [  4.4733,  22.9299,   6.7399,  -8.7571,  -7.1151,   5.1761,   4.3176,\n",
      "           6.0046, 106.0321,  -4.9620,  10.7736],\n",
      "        [ -1.8450,  -5.7365,  -4.9319,  -2.9979,  -3.2673,   7.1025,  14.7430,\n",
      "           1.2427,  -4.9620,  82.8916,  -0.3128],\n",
      "        [  4.2531,   5.1043, -19.8263,  -0.4580, -19.2185,  -6.9943,  11.8299,\n",
      "           7.9311,  10.7736,  -0.3128, 119.2813]])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.func import functional_call\n",
    "from optree import tree_map\n",
    "from optree.integration.torch import tree_ravel\n",
    "\n",
    "from posteriors.laplace import dense_ggn\n",
    "\n",
    "from scenarios import TestModel\n",
    "\n",
    "\n",
    "def normal_log_likelihood(y_pred, batch):\n",
    "    y = batch[1]\n",
    "    return (\n",
    "        Normal(y_pred, 1, validate_args=False).log_prob(y).sum()\n",
    "    )  # validate args introduces control flows not yet supported in torch.func.vmap\n",
    "\n",
    "\n",
    "def forward_m(params, b, model):\n",
    "    y_pred = functional_call(model, params, b[0])\n",
    "    return y_pred, torch.tensor([])\n",
    "\n",
    "\n",
    "def test_ggn_vmap():\n",
    "    torch.manual_seed(42)\n",
    "    model = TestModel()\n",
    "\n",
    "    xs = torch.randn(100, 10)\n",
    "    ys = model(xs)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        TensorDataset(xs, ys),\n",
    "        batch_size=20,\n",
    "    )\n",
    "\n",
    "    forward = partial(forward_m, model=model)\n",
    "\n",
    "    params = dict(model.named_parameters())\n",
    "\n",
    "    # Test inplace = False\n",
    "    transform = dense_ggn.build(forward, normal_log_likelihood)\n",
    "    laplace_state = transform.init(params)\n",
    "    laplace_state_prec_init = laplace_state.prec\n",
    "    for batch in dataloader:\n",
    "        laplace_state = transform.update(laplace_state, batch, inplace=False)\n",
    "\n",
    "    flat_params, unravel_fn = tree_ravel(params)\n",
    "\n",
    "    expected = torch.zeros((flat_params.shape[0], flat_params.shape[0]))\n",
    "    for x, y in zip(xs, ys):\n",
    "        with torch.no_grad():\n",
    "            z = forward(params, (x, y))[0]\n",
    "            J = torch.func.jacrev(lambda fp: forward(unravel_fn(fp), (x, y)))(\n",
    "                flat_params\n",
    "            )[0]\n",
    "            H = torch.func.hessian(lambda zt: normal_log_likelihood(zt, (x, y)))(z)\n",
    "            G = J.T @ H @ J\n",
    "        expected -= G\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state.prec, atol=1e-5)\n",
    "    assert not torch.allclose(laplace_state.prec, laplace_state_prec_init)\n",
    "\n",
    "    print(\"eee\\n\")\n",
    "    print(expected)\n",
    "    print(\"ppp\\n\")\n",
    "    print(laplace_state.prec)\n",
    "\n",
    "    # Also check full batch\n",
    "    laplace_state_fb = transform.init(params)\n",
    "    laplace_state_fb = transform.update(laplace_state_fb, (xs, ys))\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state_fb.prec, atol=1e-5)\n",
    "\n",
    "    # Test inplace = True\n",
    "    laplace_state = transform.init(params)\n",
    "    laplace_state_prec_init = laplace_state.prec\n",
    "    for batch in dataloader:\n",
    "        laplace_state = transform.update(laplace_state, batch, inplace=True)\n",
    "\n",
    "    assert torch.allclose(expected, laplace_state.prec, atol=1e-5)\n",
    "    assert torch.allclose(laplace_state.prec, laplace_state_prec_init)\n",
    "\n",
    "    # Test sampling\n",
    "    num_samples = 10000\n",
    "    laplace_state.prec.data += 0.1 * torch.eye(\n",
    "        flat_params.shape[0]\n",
    "    )  # regularize to ensure PSD and reduce variance\n",
    "\n",
    "    mean_copy = tree_map(lambda x: x.clone(), laplace_state.params)\n",
    "    sd_flat = torch.diag(torch.linalg.inv(laplace_state.prec)).sqrt()\n",
    "\n",
    "    samples = dense_ggn.sample(laplace_state, (num_samples,))\n",
    "\n",
    "    samples_mean = tree_map(lambda x: x.mean(dim=0), samples)\n",
    "    samples_sd = tree_map(lambda x: x.std(dim=0), samples)\n",
    "    samples_sd_flat = tree_ravel(samples_sd)[0]\n",
    "\n",
    "    for key in samples_mean:\n",
    "        assert samples[key].shape[0] == num_samples\n",
    "        assert samples[key].shape[1:] == samples_mean[key].shape\n",
    "        assert torch.allclose(samples_mean[key], laplace_state.params[key], atol=1e-1)\n",
    "        assert torch.allclose(mean_copy[key], laplace_state.params[key])\n",
    "\n",
    "    assert torch.allclose(sd_flat, samples_sd_flat, atol=1e-1)\n",
    "\n",
    "test_ggn_vmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
